{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Orchestrator-Worker Pattern\n",
    "\n",
    "This notebook implements the **Orchestrator-Worker Pattern** with **Parallel Execution**:\n",
    "\n",
    "1. **Orchestrator LLM**: Decides which worker models to call based on the question\n",
    "2. **Worker Models**: Selected models run **simultaneously** in parallel using async/await\n",
    "3. **Synthesizer LLM**: Aggregates outputs, synthesizes a final answer, and ranks the workers\n",
    "\n",
    "## Pattern Flow\n",
    "\n",
    "```\n",
    "User Question â†’ Orchestrator LLM (\"Which workers to use?\")\n",
    " â†“\n",
    "Parallel Worker API Calls (Only selected models)\n",
    " â†“\n",
    "Synthesizer LLM (Merge + Rank)\n",
    " â†“\n",
    "Final Answer + Worker Rankings\n",
    "```\n",
    "\n",
    "The **LLM orchestrator** replaces hardcoded model selection with intelligent routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Evaluation Question\n",
    "\n",
    "First, generate a challenging question to test all the worker models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Worker Configurations\n",
    "\n",
    "Prepare all available **worker** model configurations that the orchestrator can choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Worker Preparation: All Available Models\n",
    "# ==========================================\n",
    "\n",
    "def evaluator_prepare_configs():\n",
    "    \"\"\"\n",
    "    Evaluator: Gathers API keys and prepares configurations for all models.\n",
    "    Returns a list of model configurations ready for parallel execution.\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    \n",
    "    # Model 1: OpenAI\n",
    "    configs.append({\n",
    "        \"model_name\": \"gpt-5-nano\",\n",
    "        \"provider\": \"openai\",\n",
    "        \"client\": OpenAI(),\n",
    "        \"call_type\": \"chat.completions\",\n",
    "        \"extra_params\": {}\n",
    "    })\n",
    "    \n",
    "    # Model 2: Anthropic\n",
    "    configs.append({\n",
    "        \"model_name\": \"claude-sonnet-4-5\",\n",
    "        \"provider\": \"anthropic\",\n",
    "        \"client\": Anthropic(),\n",
    "        \"call_type\": \"messages.create\",\n",
    "        \"extra_params\": {\"max_tokens\": 1000}\n",
    "    })\n",
    "    \n",
    "    # Model 3: Gemini\n",
    "    configs.append({\n",
    "        \"model_name\": \"gemini-2.5-flash\",\n",
    "        \"provider\": \"gemini\",\n",
    "        \"client\": OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"),\n",
    "        \"call_type\": \"chat.completions\",\n",
    "        \"extra_params\": {}\n",
    "    })\n",
    "    \n",
    "    # Model 4: DeepSeek\n",
    "    configs.append({\n",
    "        \"model_name\": \"deepseek/deepseek-r1-0528:free\",\n",
    "        \"provider\": \"deepseek\",\n",
    "        \"client\": OpenAI(api_key=deepseek_api_key, base_url=\"https://openrouter.ai/api/v1\"),\n",
    "        \"call_type\": \"chat.completions\",\n",
    "        \"extra_params\": {}\n",
    "    })\n",
    "    \n",
    "    # Model 5: Groq\n",
    "    configs.append({\n",
    "        \"model_name\": \"openai/gpt-oss-120b\",\n",
    "        \"provider\": \"groq\",\n",
    "        \"client\": OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\"),\n",
    "        \"call_type\": \"chat.completions\",\n",
    "        \"extra_params\": {}\n",
    "    })\n",
    "    \n",
    "    # Model 6: Ollama (if available)\n",
    "    configs.append({\n",
    "        \"model_name\": \"llama3.2\",\n",
    "        \"provider\": \"ollama\",\n",
    "        \"client\": OpenAI(base_url='http://localhost:11434/v1', api_key='ollama'),\n",
    "        \"call_type\": \"chat.completions\",\n",
    "        \"extra_params\": {}\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ… Evaluator prepared {len(configs)} model configurations\")\n",
    "    return configs\n",
    "\n",
    "# Prepare global config dictionaries\n",
    "MODEL_CONFIGS = evaluator_prepare_configs()\n",
    "MODEL_CONFIGS_BY_NAME = {cfg[\"model_name\"]: cfg for cfg in MODEL_CONFIGS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Orchestrator LLM - Decide Which Workers to Use\n",
    "\n",
    "The **orchestrator LLM** analyzes the question and selects which worker models to invoke in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Orchestrator: LLM Decides Worker Selection\n",
    "# ==========================================\n",
    "\n",
    "def list_worker_tools():\n",
    "    \"\"\"Return tool descriptions for the orchestrator LLM.\"\"\"\n",
    "    tools = []\n",
    "    for cfg in MODEL_CONFIGS:\n",
    "        tools.append({\n",
    "            \"name\": cfg[\"model_name\"],\n",
    "            \"description\": f\"Call the {cfg['provider']} model {cfg['model_name']} \"\n",
    "                          \"to answer the user question.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"reason\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Why this model is useful for this query.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"reason\"]\n",
    "            }\n",
    "        })\n",
    "    return tools\n",
    "\n",
    "def build_orchestrator_prompt(user_question: str, tools: list) -> list:\n",
    "    tool_descriptions = \"\\n\".join(\n",
    "        f\"- {t['name']}: {t['description']}\" for t in tools\n",
    "    )\n",
    "\n",
    "    system_msg = textwrap.dedent(f\"\"\"\n",
    "        You are an **orchestrator** LLM in an orchestratorâ€“worker system.\n",
    "\n",
    "        You do NOT answer the user's question directly.\n",
    "        Instead, you decide which worker models to call in parallel.\n",
    "\n",
    "        Available workers:\n",
    "        {tool_descriptions}\n",
    "\n",
    "        Output STRICTLY valid JSON:\n",
    "        {{\n",
    "          \"models_to_call\": [\n",
    "            {{\"name\": \"<model_name>\", \"reason\": \"<short reason>\"}},\n",
    "            ...\n",
    "          ]\n",
    "        }}\n",
    "\n",
    "        Requirements:\n",
    "        - Choose at least 3 and at most 6 models.\n",
    "        - Use model names exactly as listed.\n",
    "        - Prefer diversity (different providers) for hard reasoning tasks.\n",
    "        - Do not include any fields other than \"models_to_call\".\n",
    "    \"\"\")\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_question},\n",
    "    ]\n",
    "\n",
    "def orchestrator_plan(user_question: str) -> list:\n",
    "    \"\"\"\n",
    "    Ask the orchestrator LLM which workers to use.\n",
    "    Returns list of selected model names.\n",
    "    \"\"\"\n",
    "    tools = list_worker_tools()\n",
    "    messages = build_orchestrator_prompt(user_question, tools)\n",
    "\n",
    "    openai = OpenAI()\n",
    "    resp = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        # model=\"gemini-1.5-flash\",\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    content = resp.choices[0].message.content\n",
    "    plan = json.loads(content)\n",
    "    selected = [m[\"name\"] for m in plan.get(\"models_to_call\", [])]\n",
    "    print(f\"ðŸŽ¯ Orchestrator selected {len(selected)} workers: {selected}\")\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Parallel Worker Execution\n",
    "\n",
    "Execute **only the selected workers** simultaneously using async/await."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Worker Execution: Async Single Model Call\n",
    "# ==========================================\n",
    "\n",
    "async def call_model_async(config: Dict[str, Any], messages: List[Dict]) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Call a single worker model asynchronously.\n",
    "    Returns (model_name, answer) or (model_name, error_message).\n",
    "    \"\"\"\n",
    "    model_name = config[\"model_name\"]\n",
    "    provider = config[\"provider\"]\n",
    "    client = config[\"client\"]\n",
    "    call_type = config[\"call_type\"]\n",
    "    extra_params = config[\"extra_params\"]\n",
    "    \n",
    "    try:\n",
    "        if provider == \"anthropic\":\n",
    "            # Anthropic uses a different API structure\n",
    "            response = await asyncio.to_thread(\n",
    "                client.messages.create,\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                **extra_params\n",
    "            )\n",
    "            answer = response.content[0].text\n",
    "        else:\n",
    "            # OpenAI-compatible APIs\n",
    "            response = await asyncio.to_thread(\n",
    "                client.chat.completions.create,\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                **extra_params\n",
    "            )\n",
    "            answer = response.choices[0].message.content\n",
    "        \n",
    "        print(f\"âœ… {model_name} completed\")\n",
    "        return model_name, answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error calling {model_name}: {str(e)}\"\n",
    "        print(f\"âŒ {error_msg}\")\n",
    "        return model_name, error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Parallel Worker Execution\n",
    "# ==========================================\n",
    "\n",
    "def format_bytes(size: int) -> str:\n",
    "    \"\"\"Format bytes into a human-readable string (B, KB, MB).\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB']:\n",
    "        if size < 1024.0:\n",
    "            return f\"{size:.2f} {unit}\"\n",
    "        size /= 1024.0\n",
    "    return f\"{size:.2f} GB\"\n",
    "\n",
    "async def execute_models_in_parallel(configs: List[Dict[str, Any]], messages: List[Dict]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Execute selected worker models in parallel.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸš€ Starting parallel execution of {len(configs)} selected workers...\\n\")\n",
    "    \n",
    "    table_rows = []\n",
    "    competitors = []\n",
    "    answers = []\n",
    "    \n",
    "    async def call_with_metrics(config):\n",
    "        model_name = config.get(\"model_name\", \"Unknown\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            _, answer = await call_model_async(config, messages)\n",
    "            end_time = datetime.now()\n",
    "            \n",
    "            if isinstance(answer, str) and answer.startswith(\"Error\"):\n",
    "                status = \"âŒ Error\"\n",
    "                out_size = 0\n",
    "            else:\n",
    "                status = \"âœ… Success\"\n",
    "                out_size = len(str(answer).encode('utf-8'))\n",
    "                \n",
    "        except Exception as e:\n",
    "            end_time = datetime.now()\n",
    "            status = \"âŒ Error\"\n",
    "            answer = str(e)\n",
    "            out_size = 0\n",
    "\n",
    "        # Calculate duration\n",
    "        duration = end_time - start_time\n",
    "        total_seconds = int(duration.total_seconds())\n",
    "        mm, ss = divmod(total_seconds, 60)\n",
    "        hh, mm = divmod(mm, 60)\n",
    "        dur_str = f\"{hh:02d}:{mm:02d}:{ss:02d}\" if hh > 0 else f\"{mm:02d}:{ss:02d}\"\n",
    "\n",
    "        # Store metrics for table\n",
    "        table_rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"status\": status,\n",
    "            \"start\": start_time.strftime(\"%H:%M:%S\"),\n",
    "            \"end\": end_time.strftime(\"%H:%M:%S\"),\n",
    "            \"duration\": dur_str,\n",
    "            \"size\": format_bytes(out_size)\n",
    "        })\n",
    "        \n",
    "        return model_name, answer, status\n",
    "\n",
    "    # Run tasks in parallel\n",
    "    tasks = [call_with_metrics(config) for config in configs]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Process final lists\n",
    "    for model_name, answer, status in results:\n",
    "        if status == \"âœ… Success\":\n",
    "            competitors.append(model_name)\n",
    "            answers.append(answer)\n",
    "\n",
    "    # Print Tabular Output\n",
    "    header = f\"{'Model':<25} {'Status':<10} {'Start':<10} {'End':<10} {'Duration':<10} {'Size':<12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for row in table_rows:\n",
    "        print(f\"{row['model']:<25} {row['status']:<10} {row['start']:<10} {row['end']:<10} {row['duration']:<10} {row['size']:<12}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Completed. {len(competitors)}/{len(configs)} workers successful.\")\n",
    "    return competitors, answers\n",
    "\n",
    "async def mock_execute_models_in_parallel(configs: List[Dict[str, Any]]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Mocks parallel API calls to display timing and size metrics in a table.\n",
    "    No actual API calls are made.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸš€ Starting MOCK execution of {len(configs)} models...\\n\")\n",
    "    \n",
    "    table_rows = []\n",
    "    competitors = []\n",
    "    answers = []\n",
    "\n",
    "    async def mock_api_call(config):\n",
    "        model_name = config.get(\"model_name\", \"Unknown-Model\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Simulate varying network latency (0.5 to 2.5 seconds)\n",
    "        await asyncio.sleep(random.uniform(0.5, 2.5))\n",
    "        \n",
    "        # Randomly decide if this mock call \"fails\" (10% chance)\n",
    "        is_success = random.random() > 0.1\n",
    "        \n",
    "        if is_success:\n",
    "            status = \"âœ… Success\"\n",
    "            # Mock a response string of random length\n",
    "            mock_answer = \"Mock response data \" * random.randint(5, 500)\n",
    "            out_size = len(mock_answer.encode('utf-8'))\n",
    "        else:\n",
    "            status = \"âŒ Error\"\n",
    "            mock_answer = \"Error: Mocked API failure\"\n",
    "            out_size = 0\n",
    "            \n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        # Calculate duration in mm:ss or hh:mm:ss\n",
    "        duration = end_time - start_time\n",
    "        total_seconds = int(duration.total_seconds())\n",
    "        mm, ss = divmod(total_seconds, 60)\n",
    "        hh, mm = divmod(mm, 60)\n",
    "        dur_str = f\"{hh:02d}:{mm:02d}:{ss:02d}\" if hh > 0 else f\"{mm:02d}:{ss:02d}\"\n",
    "\n",
    "        # Record metrics for the final table\n",
    "        metrics = {\n",
    "            \"model\": model_name,\n",
    "            \"status\": status,\n",
    "            \"start\": start_time.strftime(\"%H:%M:%S\"),\n",
    "            \"end\": end_time.strftime(\"%H:%M:%S\"),\n",
    "            \"duration\": dur_str,\n",
    "            \"size\": format_bytes(out_size)\n",
    "        }\n",
    "        \n",
    "        return model_name, mock_answer, status, metrics\n",
    "\n",
    "    # Execute mock tasks in parallel\n",
    "    tasks = [mock_api_call(config) for config in configs]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Prepare table headers\n",
    "    header = f\"{'Model':<20} {'Status':<10} {'Start':<10} {'End':<10} {'Duration':<10} {'Size':<12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    # Output rows and collect final success data\n",
    "    for model_name, answer, status, row in results:\n",
    "        print(f\"{row['model']:<20} {row['status']:<10} {row['start']:<10} {row['end']:<10} {row['duration']:<10} {row['size']:<12}\")\n",
    "        if status == \"âœ… Success\":\n",
    "            competitors.append(model_name)\n",
    "            answers.append(answer)\n",
    "\n",
    "    print(f\"\\nâœ… Completed. {len(competitors)}/{len(configs)} models simulated successfully.\")\n",
    "    return competitors, answers\n",
    "\n",
    "async def execute_selected_models(model_names: list, messages: list):\n",
    "    \"\"\"Execute only the orchestrator-selected workers.\"\"\"\n",
    "    selected_configs = [MODEL_CONFIGS_BY_NAME[m] for m in model_names]\n",
    "    return await execute_models_in_parallel(selected_configs, messages)\n",
    "\n",
    "async def mock_execute_selected_models(model_names: list):\n",
    "    \"\"\"Execute only the orchestrator-selected workers.\"\"\"\n",
    "    selected_configs = [MODEL_CONFIGS_BY_NAME[m] for m in model_names]\n",
    "    return await mock_execute_models_in_parallel(selected_configs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Orchestrated Pipeline\n",
    "\n",
    "**Full end-to-end execution**: Orchestrator â†’ Workers â†’ Synthesizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# FULL ORCHESTRATED PIPELINE\n",
    "# ==========================================\n",
    "\n",
    "user_question = question  # From Step 1\n",
    "messages = [{\"role\": \"user\", \"content\": user_question}]\n",
    "\n",
    "# 1) Orchestrator chooses workers\n",
    "selected_models = orchestrator_plan(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Mock execute chosen workers\n",
    "competitors, answers = await mock_execute_selected_models(selected_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Run chosen workers in parallel\n",
    "competitors, answers = await execute_selected_models(selected_models, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Synthesizer LLM - Merge + Rank\n",
    "\n",
    "The **synthesizer LLM** creates a final answer and ranks the worker models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Synthesizer: Merge Outputs + Rank Workers\n",
    "# ==========================================\n",
    "\n",
    "def aggregator_format_outputs(competitors: List[str], answers: List[str]) -> str:\n",
    "    \"\"\"Format worker outputs for the synthesizer.\"\"\"\n",
    "    together = \"\"\n",
    "    for index, answer in enumerate(answers):\n",
    "        together += f\"# Response from {competitors[index]}\\n\\n\"\n",
    "        together += answer + \"\\n\\n\"\n",
    "    return together\n",
    "\n",
    "def build_synthesizer_prompt(question: str, competitors: list, answers: list) -> list:\n",
    "    formatted = aggregator_format_outputs(competitors, answers)\n",
    "    num_workers = len(competitors)\n",
    "    \n",
    "    system_msg = f\"\"\"\n",
    "    You are evaluating EXACTLY {num_workers} worker responses.\n",
    "    \n",
    "    CRITICAL: There are ONLY {num_workers} competitors numbered 1-{num_workers}.\n",
    "    \n",
    "    Tasks:\n",
    "    1. Synthesize final answer from these {num_workers} responses\n",
    "    2. Rank ONLY these {num_workers} responses (indices 1-{num_workers})\n",
    "    \n",
    "    Output STRICTLY valid JSON:\n",
    "    {{\n",
    "      \"final_answer\": \"your synthesized answer\",\n",
    "      \"rankings\": [\n",
    "        {{\"competitor_index\": N, \"reason\": \"why N is good\"}}  // N is 1-{num_workers}\n",
    "        // EXACTLY {num_workers} entries, no more, no less\n",
    "      ]\n",
    "    }}\n",
    "    \n",
    "    Responses ({num_workers} total):\n",
    "    {formatted}\n",
    "    \"\"\"\n",
    "    \n",
    "    return [{\"role\": \"system\", \"content\": system_msg}]\n",
    "\n",
    "def run_synthesizer(question: str, competitors: list, answers: list):\n",
    "    \"\"\"Run the synthesizer LLM.\"\"\"\n",
    "    messages = build_synthesizer_prompt(question, competitors, answers)\n",
    "    openai = OpenAI()\n",
    "    resp = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    data = json.loads(resp.choices[0].message.content)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# FINAL SYNTHESIS AND RANKINGS\n",
    "# ==========================================\n",
    "\n",
    "# Run synthesizer\n",
    "synth = run_synthesizer(question, competitors, answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, r in enumerate(synth[\"rankings\"], start=1):\n",
    "    name = competitors[r[\"competitor_index\"] - 1]\n",
    "    print(f\"{idx}. {name} â€” {r['reason']}\")\n",
    "\n",
    "print(f\"\\nâœ… Pipeline complete! Orchestrator â†’ {len(selected_models)} Workers â†’ Synthesizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMMEDIATE DEBUG\n",
    "print(\"COMPETITORS:\", competitors)\n",
    "print(\"NUM COMPETITORS:\", len(competitors))\n",
    "print(\"RAW SYNTH:\", json.dumps(synth, indent=2))\n",
    "print(\"RANKINGS:\", [r.get(\"competitor_index\") for r in synth.get(\"rankings\", [])])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
