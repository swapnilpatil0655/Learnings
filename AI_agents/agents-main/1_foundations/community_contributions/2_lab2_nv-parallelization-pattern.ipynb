{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Parallelization and Evaluator-Optimizer Pattern\n",
        "\n",
        "This notebook implements the **Evaluator-Optimizer Pattern** with **Parallelization**:\n",
        "\n",
        "1. **Evaluator**: Gathers API keys and prepares model configurations\n",
        "2. **Parallel Execution**: All models run simultaneously using async/await\n",
        "3. **Aggregator**: Collects and formats all outputs for evaluation\n",
        "4. **Final Evaluator**: Judge model ranks all responses from best to worst\n",
        "\n",
        "## Pattern Flow\n",
        "\n",
        "```\n",
        "Evaluator (API Keys & Configs) \n",
        "    â†“\n",
        "Parallel API Calls (All models run simultaneously)\n",
        "    â†“\n",
        "Aggregator (Collect & Format outputs)\n",
        "    â†“\n",
        "Final Evaluator (Judge ranks all outputs)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
        "\n",
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from anthropic import Anthropic\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Always remember to do this!\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the key prefixes to help with any debugging\n",
        "\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
        "groq_api_key = os.getenv('GROQ_API_KEY')\n",
        "\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "    \n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
        "else:\n",
        "    print(\"Anthropic API Key not set (and this is optional)\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
        "else:\n",
        "    print(\"Google API Key not set (and this is optional)\")\n",
        "\n",
        "if deepseek_api_key:\n",
        "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
        "else:\n",
        "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
        "\n",
        "if groq_api_key:\n",
        "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
        "else:\n",
        "    print(\"Groq API Key not set (and this is optional)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate Question\n",
        "\n",
        "First, we'll generate a challenging question to ask all the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
        "request += \"Answer only with the question, no explanation.\"\n",
        "messages = [{\"role\": \"user\", \"content\": request}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai = OpenAI()\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=messages,\n",
        ")\n",
        "question = response.choices[0].message.content\n",
        "print(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the question for all models\n",
        "competitors = []\n",
        "answers = []\n",
        "messages = [{\"role\": \"user\", \"content\": question}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Evaluator - Prepare API Keys and Configurations\n",
        "\n",
        "The **Evaluator** gathers API keys and prepares configurations for all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Evaluator: Gathers API keys and prepares model configurations\n",
        "# ==========================================\n",
        "# This function prepares all model configurations ready for parallel execution\n",
        "\n",
        "def evaluator_prepare_configs():\n",
        "    \"\"\"\n",
        "    Evaluator: Gathers API keys and prepares configurations for all models.\n",
        "    Returns a list of model configurations ready for parallel execution.\n",
        "    \"\"\"\n",
        "    configs = []\n",
        "    \n",
        "    # Model 1: OpenAI\n",
        "    configs.append({\n",
        "        \"model_name\": \"gpt-5-nano\",\n",
        "        \"provider\": \"openai\",\n",
        "        \"client\": OpenAI(),\n",
        "        \"call_type\": \"chat.completions\",\n",
        "        \"extra_params\": {}\n",
        "    })\n",
        "    \n",
        "    # Model 2: Anthropic\n",
        "    configs.append({\n",
        "        \"model_name\": \"claude-sonnet-4-5\",\n",
        "        \"provider\": \"anthropic\",\n",
        "        \"client\": Anthropic(),\n",
        "        \"call_type\": \"messages.create\",\n",
        "        \"extra_params\": {\"max_tokens\": 1000}\n",
        "    })\n",
        "    \n",
        "    # Model 3: Gemini\n",
        "    configs.append({\n",
        "        \"model_name\": \"gemini-2.5-flash\",\n",
        "        \"provider\": \"gemini\",\n",
        "        \"client\": OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"),\n",
        "        \"call_type\": \"chat.completions\",\n",
        "        \"extra_params\": {}\n",
        "    })\n",
        "    \n",
        "    # Model 4: DeepSeek\n",
        "    configs.append({\n",
        "        \"model_name\": \"deepseek/deepseek-r1-0528:free\",\n",
        "        \"provider\": \"deepseek\",\n",
        "        \"client\": OpenAI(api_key=deepseek_api_key, base_url=\"https://openrouter.ai/api/v1\"),\n",
        "        \"call_type\": \"chat.completions\",\n",
        "        \"extra_params\": {}\n",
        "    })\n",
        "    \n",
        "    # Model 5: Groq\n",
        "    configs.append({\n",
        "        \"model_name\": \"openai/gpt-oss-120b\",\n",
        "        \"provider\": \"groq\",\n",
        "        \"client\": OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\"),\n",
        "        \"call_type\": \"chat.completions\",\n",
        "        \"extra_params\": {}\n",
        "    })\n",
        "    \n",
        "    # Model 6: Ollama (if available)\n",
        "    configs.append({\n",
        "        \"model_name\": \"llama3.2\",\n",
        "        \"provider\": \"ollama\",\n",
        "        \"client\": OpenAI(base_url='http://localhost:11434/v1', api_key='ollama'),\n",
        "        \"call_type\": \"chat.completions\",\n",
        "        \"extra_params\": {}\n",
        "    })\n",
        "    \n",
        "    print(f\"âœ… Evaluator prepared {len(configs)} model configurations\")\n",
        "    return configs\n",
        "\n",
        "# Prepare all configurations\n",
        "model_configs = evaluator_prepare_configs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Parallel Execution - Call All Models Simultaneously\n",
        "\n",
        "All models are called **in parallel** using async/await, making the process much faster than sequential calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Async function to call a single model\n",
        "# ==========================================\n",
        "\n",
        "async def call_model_async(config: Dict[str, Any], messages: List[Dict]) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Call a single model asynchronously. Returns (model_name, answer) or (model_name, error_message).\n",
        "    \"\"\"\n",
        "    model_name = config[\"model_name\"]\n",
        "    provider = config[\"provider\"]\n",
        "    client = config[\"client\"]\n",
        "    call_type = config[\"call_type\"]\n",
        "    extra_params = config[\"extra_params\"]\n",
        "    \n",
        "    try:\n",
        "        if provider == \"anthropic\":\n",
        "            # Anthropic uses a different API structure\n",
        "            response = await asyncio.to_thread(\n",
        "                client.messages.create,\n",
        "                model=model_name,\n",
        "                messages=messages,\n",
        "                **extra_params\n",
        "            )\n",
        "            answer = response.content[0].text\n",
        "        else:\n",
        "            # OpenAI-compatible APIs\n",
        "            response = await asyncio.to_thread(\n",
        "                client.chat.completions.create,\n",
        "                model=model_name,\n",
        "                messages=messages,\n",
        "                **extra_params\n",
        "            )\n",
        "            answer = response.choices[0].message.content\n",
        "        \n",
        "        print(f\"âœ… {model_name} completed\")\n",
        "        return model_name, answer\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error calling {model_name}: {str(e)}\"\n",
        "        print(f\"âŒ {error_msg}\")\n",
        "        return model_name, error_msg\n",
        "\n",
        "# ==========================================\n",
        "# Parallel execution function\n",
        "# ==========================================\n",
        "\n",
        "def format_bytes(size: int) -> str:\n",
        "    \"\"\"Format bytes into a human-readable string (B, KB, MB).\"\"\"\n",
        "    for unit in ['B', 'KB', 'MB']:\n",
        "        if size < 1024.0:\n",
        "            return f\"{size:.2f} {unit}\"\n",
        "        size /= 1024.0\n",
        "    return f\"{size:.2f} GB\"\n",
        "\n",
        "async def execute_models_in_parallel(configs: List[Dict[str, Any]], messages: List[Dict]) -> Tuple[List[str], List[str]]:\n",
        "    # Overall execution start\n",
        "    print(f\"\\nðŸš€ Starting parallel execution of {len(configs)} models...\\n\")\n",
        "    \n",
        "    # Track data for the table\n",
        "    table_rows = []\n",
        "    competitors = []\n",
        "    answers = []\n",
        "    \n",
        "    async def call_with_metrics(config):\n",
        "        model_name = config.get(\"model_name\", \"Unknown\")\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        try:\n",
        "            # Assumes call_model_async returns (model_name, answer)\n",
        "            _, answer = await call_model_async(config, messages)\n",
        "            end_time = datetime.now()\n",
        "            \n",
        "            # Check for error strings inside the success path\n",
        "            if isinstance(answer, str) and answer.startswith(\"Error\"):\n",
        "                status = \"âŒ Error\"\n",
        "                out_size = 0\n",
        "            else:\n",
        "                status = \"âœ… Success\"\n",
        "                out_size = len(str(answer).encode('utf-8'))\n",
        "                \n",
        "        except Exception as e:\n",
        "            end_time = datetime.now()\n",
        "            status = \"âŒ Error\"\n",
        "            answer = str(e)\n",
        "            out_size = 0\n",
        "\n",
        "        # Calculate duration\n",
        "        duration = end_time - start_time\n",
        "        total_seconds = int(duration.total_seconds())\n",
        "        mm, ss = divmod(total_seconds, 60)\n",
        "        hh, mm = divmod(mm, 60)\n",
        "        dur_str = f\"{hh:02d}:{mm:02d}:{ss:02d}\" if hh > 0 else f\"{mm:02d}:{ss:02d}\"\n",
        "\n",
        "        # Store metrics for table\n",
        "        table_rows.append({\n",
        "            \"model\": model_name,\n",
        "            \"status\": status,\n",
        "            \"start\": start_time.strftime(\"%H:%M:%S\"),\n",
        "            \"end\": end_time.strftime(\"%H:%M:%S\"),\n",
        "            \"duration\": dur_str,\n",
        "            \"size\": format_bytes(out_size)\n",
        "        })\n",
        "        \n",
        "        return model_name, answer, status\n",
        "\n",
        "    # Run tasks in parallel\n",
        "    tasks = [call_with_metrics(config) for config in configs]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Process final lists\n",
        "    for model_name, answer, status in results:\n",
        "        if status == \"âœ… Success\":\n",
        "            competitors.append(model_name)\n",
        "            answers.append(answer)\n",
        "\n",
        "    # Print Tabular Output\n",
        "    header = f\"{'Model':<20} {'Status':<10} {'Start':<10} {'End':<10} {'Duration':<10} {'Size':<12}\"\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for row in table_rows:\n",
        "        print(f\"{row['model']:<20} {row['status']:<10} {row['start']:<10} {row['end']:<10} {row['duration']:<10} {row['size']:<12}\")\n",
        "    \n",
        "    print(f\"\\nâœ… Completed. {len(competitors)}/{len(configs)} models successful.\")\n",
        "    return competitors, answers\n",
        "\n",
        "async def mock_execute_models_in_parallel(configs: List[Dict[str, Any]]) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Mocks parallel API calls to display timing and size metrics in a table.\n",
        "    No actual API calls are made.\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸš€ Starting MOCK execution of {len(configs)} models...\\n\")\n",
        "    \n",
        "    table_rows = []\n",
        "    competitors = []\n",
        "    answers = []\n",
        "\n",
        "    async def mock_api_call(config):\n",
        "        model_name = config.get(\"model_name\", \"Unknown-Model\")\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Simulate varying network latency (0.5 to 2.5 seconds)\n",
        "        await asyncio.sleep(random.uniform(0.5, 2.5))\n",
        "        \n",
        "        # Randomly decide if this mock call \"fails\" (10% chance)\n",
        "        is_success = random.random() > 0.1\n",
        "        \n",
        "        if is_success:\n",
        "            status = \"âœ… Success\"\n",
        "            # Mock a response string of random length\n",
        "            mock_answer = \"Mock response data \" * random.randint(5, 500)\n",
        "            out_size = len(mock_answer.encode('utf-8'))\n",
        "        else:\n",
        "            status = \"âŒ Error\"\n",
        "            mock_answer = \"Error: Mocked API failure\"\n",
        "            out_size = 0\n",
        "            \n",
        "        end_time = datetime.now()\n",
        "        \n",
        "        # Calculate duration in mm:ss or hh:mm:ss\n",
        "        duration = end_time - start_time\n",
        "        total_seconds = int(duration.total_seconds())\n",
        "        mm, ss = divmod(total_seconds, 60)\n",
        "        hh, mm = divmod(mm, 60)\n",
        "        dur_str = f\"{hh:02d}:{mm:02d}:{ss:02d}\" if hh > 0 else f\"{mm:02d}:{ss:02d}\"\n",
        "\n",
        "        # Record metrics for the final table\n",
        "        metrics = {\n",
        "            \"model\": model_name,\n",
        "            \"status\": status,\n",
        "            \"start\": start_time.strftime(\"%H:%M:%S\"),\n",
        "            \"end\": end_time.strftime(\"%H:%M:%S\"),\n",
        "            \"duration\": dur_str,\n",
        "            \"size\": format_bytes(out_size)\n",
        "        }\n",
        "        \n",
        "        return model_name, mock_answer, status, metrics\n",
        "\n",
        "    # Execute mock tasks in parallel\n",
        "    tasks = [mock_api_call(config) for config in configs]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Prepare table headers\n",
        "    header = f\"{'Model':<20} {'Status':<10} {'Start':<10} {'End':<10} {'Duration':<10} {'Size':<12}\"\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    # Output rows and collect final success data\n",
        "    for model_name, answer, status, row in results:\n",
        "        print(f\"{row['model']:<20} {row['status']:<10} {row['start']:<10} {row['end']:<10} {row['duration']:<10} {row['size']:<12}\")\n",
        "        if status == \"âœ… Success\":\n",
        "            competitors.append(model_name)\n",
        "            answers.append(answer)\n",
        "\n",
        "    print(f\"\\nâœ… Completed. {len(competitors)}/{len(configs)} models simulated successfully.\")\n",
        "    return competitors, answers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Mock API Calls ---\n",
        "competitors, answers = await mock_execute_models_in_parallel(model_configs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute all models in parallel using the async functions\n",
        "competitors, answers = await execute_models_in_parallel(model_configs, messages)\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optionally - Display the answers\n",
        "# for model_name, answer in zip(competitors, answers):\n",
        "#     display(Markdown(f\"### {model_name}\\n\\n{answer}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Aggregator - Collect and Format Outputs\n",
        "\n",
        "The **Aggregator** collects all model outputs and formats them for the final evaluator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Aggregator: Collect outputs and format for evaluation\n",
        "# ==========================================\n",
        "# The Aggregator collects all model outputs and prepares them\n",
        "# for the final Evaluator (judge) that will rank the responses\n",
        "\n",
        "def aggregator_format_outputs(competitors: List[str], answers: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Aggregator: Collects all model outputs and formats them for evaluation.\n",
        "    Returns a formatted string ready for the judge/evaluator.\n",
        "    \"\"\"\n",
        "    together = \"\"\n",
        "    for index, answer in enumerate(answers):\n",
        "        together += f\"# Response from competitor {index+1}\\n\\n\"\n",
        "        together += answer + \"\\n\\n\"\n",
        "    return together\n",
        "\n",
        "# Use the aggregator to format all outputs\n",
        "together = aggregator_format_outputs(competitors, answers)\n",
        "print(f\"âœ… Aggregator collected and formatted {len(competitors)} model responses\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Final Evaluator - Judge and Rank All Outputs\n",
        "\n",
        "The **Final Evaluator** (Judge) evaluates all aggregated responses and ranks them from best to worst."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Final Evaluator: Judge and Rank All Outputs\n",
        "# ==========================================\n",
        "# The final Evaluator (Judge) model evaluates all aggregated responses\n",
        "# and ranks them from best to worst\n",
        "\n",
        "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
        "Each model has been given this question:\n",
        "\n",
        "{question}\n",
        "\n",
        "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
        "Respond with JSON, and only JSON, with the following format:\n",
        "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
        "\n",
        "Here are the responses from each competitor:\n",
        "\n",
        "{together}\n",
        "\n",
        "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n",
        "\n",
        "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Final Evaluator Call: Judge Ranks All Outputs\n",
        "# ==========================================\n",
        "# The Evaluator (Judge) model evaluates and ranks all model responses\n",
        "\n",
        "openai = OpenAI()\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=judge_messages,\n",
        ")\n",
        "results = response.choices[0].message.content\n",
        "print(\"âœ… Final Evaluator (Judge) completed ranking:\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse and display the final rankings\n",
        "results_dict = json.loads(results)\n",
        "ranks = results_dict[\"results\"]\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL RANKINGS\")\n",
        "print(\"=\"*50)\n",
        "for index, result in enumerate(ranks):\n",
        "    competitor = competitors[int(result)-1]\n",
        "    print(f\"Rank {index+1}: {competitor}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
