{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ce93ad",
   "metadata": {},
   "source": [
    "# Designing a workflow agent pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749cc73c",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa188cb",
   "metadata": {},
   "source": [
    "To build a robust \"Task-Optimized Solver\" that maximizes answer quality by combining three Agentic Patterns: Parallelization, Evaluation, and Routing. The goal is to first generate the highest-quality problem possible through competition, and then dynamically assign that problem to the specific AI model best suited to solve it.\n",
    "\n",
    "**Workflow Steps**\n",
    "\n",
    "Phase 1: Generation (Parallelization)\n",
    "\n",
    "- Fan-Out: Simultaneously prompt three different models to generate a challenging problem (math, reasoning, or coding).\n",
    "\n",
    "- Aggregator: Collect the three candidate problems into a single list.\n",
    "\n",
    "- Judge: Use a model (e.g., GPT-4o) to evaluate the candidates and output only the single best problem.\n",
    "\n",
    "Phase 2: Classification (Evaluator). \n",
    "\n",
    "- Analysis: Classify the question into a topic.\n",
    "\n",
    "- Decision: The evaluator classifies the problem into one of three domains: MATH, REASONING, or CODE.\n",
    "\n",
    "Phase 3: Specialized Execution (Router).\n",
    "\n",
    "- Routing: Automatically direct the problem to the domain specialist: * Math: Send to specific model. * Reasoning: Send to another. * Code: Send to another.\n",
    "\n",
    "- Final Output: Display the specialized solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c98d7",
   "metadata": {},
   "source": [
    "## Set up the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6199e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e6e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the API key variables\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b494e22",
   "metadata": {},
   "source": [
    "## Phase 1: Generation (Parallelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the request\n",
    "\n",
    "request = \"Please design a single, difficult question to evaluate the specifyc intelligence of an LLM.\"\n",
    "request += \"You must independently choose to make it one of these three types:\"\n",
    "request += \"1) A Mathematical Problem, 2) A logical Reasoning Puzzle, or 3) A coding Challenge.\"\n",
    "request += \"Do not tell me wich category you chose. Just output the question itself\"\n",
    "\n",
    "# Create the messages object the we will send to all competitors\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": request\n",
    "}]\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad266f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists\n",
    "competitors = []\n",
    "questions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da961ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini call\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash-preview-09-2025\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "questions.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq call\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"openai/gpt-oss-120b\"\n",
    "\n",
    "response = groq.chat.completions.create(\n",
    "    model=model_name, \n",
    "    messages=messages\n",
    "    )\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "questions.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7def008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama call\n",
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(\n",
    "    model=model_name, \n",
    "    messages=messages\n",
    "    )\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "questions.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ad5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(competitors)\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for the judge \n",
    "# we zip the lists together\n",
    "\n",
    "for competitor, question in zip(competitors, questions):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation building\n",
    "\n",
    "# We define a string variable to hold all the options\n",
    "candidates_questions = \"\"\n",
    "\n",
    "# We use enumerate to assign IDs automatically\n",
    "for index, question in enumerate(questions):\n",
    "    # index is 0, then 1, then 2 ...\n",
    "    # answer is the actual text\n",
    "    # we use {index+a} because humans count from 1, but Python counts from 0\n",
    "    candidates_questions += f\"# Question proposed from competitor {index+1}\\n\\n\"\n",
    "    candidates_questions += question + \"\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidates_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the judgeÂ´s system prompt\n",
    "judge = f\"\"\"\n",
    "You are an expert in IQ detection level.\n",
    "Here are 3 candidates question generated by AI models to evaluate their intelligence:\n",
    "\n",
    "{candidates_questions}\n",
    "\n",
    "Task: Analyze these questions. Select the SINGLE most accurate question to measure the intelligence level of an LLM.\n",
    "\n",
    "Return your decision in this exact JSON format, response ONLY in JSON:\n",
    "{{\n",
    "    \"winner_id\": <number 1, 2 or 3>,\n",
    "    \"reason\": \"<short explanation in PLAIN ENGLISH ONLY. Do not use LaTeX, backslashes, or markdown>\"\n",
    "}}\n",
    "\"\"\"\n",
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge messages\n",
    "judge_messages = [{\n",
    "    \"role\":\"user\",\n",
    "    \"content\": judge\n",
    "    }]\n",
    "\n",
    "print(judge_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the judge\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash-preview-09-2025\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=judge_messages)\n",
    "winner_text = response.choices[0].message.content\n",
    "\n",
    "print(winner_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709eaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the output\n",
    "winner_text = winner_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "try:\n",
    "    # Parse the JSON \n",
    "    winner_dict = json.loads(winner_text)\n",
    "    \n",
    "    # Get the ID\n",
    "    winning_id = int(winner_dict[\"winner_id\"])\n",
    "    \n",
    "    # RETRIEVE THE QUESTION FROM YOUR LIST\n",
    "    selected_question = questions[winning_id - 1]\n",
    "    \n",
    "    print(f\"Winner ID: {winning_id}\")\n",
    "    print(f\"Reason: {winner_dict['reason']}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Selected Question (Safe Retrieve):\\n{selected_question[:200]}...\")\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON Error: {e}\")\n",
    "    print(f\"Raw Output: {winner_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58505a6",
   "metadata": {},
   "source": [
    "## Phase 2: Classification (Evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad92a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Router Prompt\n",
    "\n",
    "router_prompt = f\"\"\"\n",
    "You are an intelligent classifier agent.\n",
    "Classify the following question into exactly one of these three categories:\n",
    "- MATH\n",
    "- CODE\n",
    "- Reasoning\n",
    "\n",
    "Question:\n",
    "\"{selected_question}\"\n",
    "\n",
    "Output ONLY the category name. Do not explain.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Router (using Gemini for free)\n",
    "\n",
    "# Router messages\n",
    "router_messages = [{\n",
    "    \"role\":\"user\",\n",
    "    \"content\": router_prompt\n",
    "    }]\n",
    "\n",
    "print(router_messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the router\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash-preview-09-2025\"\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "    model=model_name, \n",
    "    messages=router_messages\n",
    "    )\n",
    "\n",
    "topic = response.choices[0].message.content\n",
    "\n",
    "print(f\"Router decision: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a94151",
   "metadata": {},
   "source": [
    "## Phase 3: Specialized Execution (Router)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a98872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use an IF/ELSE block to direct traffic base on the \"topic\" variables\n",
    "\n",
    "if \"MATH\" in topic:\n",
    "    print(\"Routing to math expert LLM ...\")\n",
    "\n",
    "    # Create a specific prompt for the Math Expert\n",
    "    math_prompt = f\"You are a mathematician. Solve this problem step-by-step, showing all work:\\n\\n{selected_question}\"\n",
    "\n",
    "    math_messages = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\": math_prompt\n",
    "        }]\n",
    "\n",
    "    gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "    model_name = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "    response = gemini.chat.completions.create(\n",
    "        model= model_name, \n",
    "        messages= math_messages\n",
    "        )\n",
    "\n",
    "    math_response = response.choices[0].message.content\n",
    "\n",
    "    display(Markdown(selected_question))\n",
    "    display(Markdown(math_response))\n",
    "\n",
    "elif \"CODE\" in topic:\n",
    "    print(\"Routing to code expert LLM ...\")\n",
    "\n",
    "    # Create a specific prompt for the CodeExpert\n",
    "    code_prompt = f\"You are a Senior Python Developer. Write efficient code to solve this:\\n\\n{selected_question}\"\n",
    "\n",
    "    code_messages = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\": code_prompt\n",
    "        }]\n",
    "\n",
    "    ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "    model_name = \"llama3.2\"\n",
    "\n",
    "    response = ollama.chat.completions.create(\n",
    "        model= model_name, \n",
    "        messages= code_messages\n",
    "        )\n",
    "\n",
    "    code_response = response.choices[0].message.content\n",
    "\n",
    "    display(Markdown(selected_question))\n",
    "    display(Markdown(code_response))\n",
    "\n",
    "elif \"REASONING\" in topic:\n",
    "    print(\"Routing to reasoning expert LLM ...\")\n",
    "\n",
    "    # Create a specific prompt for the Reasoning Expert\n",
    "    reasoning_prompt = f\"Solve this logic puzzle clearly:\\n\\n{selected_question}\"\n",
    "\n",
    "    reasoning_messages = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\": reasoning_prompt\n",
    "        }]\n",
    "\n",
    "    ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "    model_name = \"llama3.2\"\n",
    "\n",
    "    response = ollama.chat.completions.create(\n",
    "        model= model_name, \n",
    "        messages= reasoning_messages\n",
    "        )\n",
    "\n",
    "    reasoning_response = response.choices[0].message.content\n",
    "\n",
    "    display(Markdown(selected_question))\n",
    "    display(Markdown(reasoning_response))\n",
    "\n",
    "else:\n",
    "    print(f\"Error: Router returned unknow topic '{topic}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade9a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
